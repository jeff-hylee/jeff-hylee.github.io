---
layout: post
title:  "Automatic Differentiation"
date:   2021-05-20 23:26:00 -0400
categories: ML general 
permalink: /:categories/:title:output_ext
uselatex: true
---
A computational method/technique to evaluate derivatives numerically. It is based on applying the chain-rule iteratively. This is relevant to modern ML because it is used for implementation of gradient descent; more specifically, the backwards propagation.

There are 2 "modes" in which AD can be performed: **forward accumulation** and **reverse accumulation**.

Fundamentally:

- forward accumulation ⇒ partial derivative of each sub-expression with respect to the independent variable $$x$$
- reverse accumulation ⇒ partial derivative of the whole expression with respect to each sub-expression

For both modes, we will use the same function (as an example)

$$f(x) = x_1x_2+sin(x_1) \\ = w_1w_2 +  sin(w_1) \\ = w_3 + w_4 \\ = w_5$$

## Forward accumulation

In forward accumulation AD, we rely on computing **partial derivatives of each sub-expression with respect to the independent variable ($$x$$)** over the computational graph. This means that each of the nodes in the computation graph holds the information: $$\dot{w_i} = \partial w_i/\partial x$$.

![https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png](https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png)

*Fig 1. Forward accumulation AD. Image source: [wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation)*

This means:

- $$\dot{w_1} = \frac{\partial w_1}{\partial x} = 1$$ (seed)
- $$\dot{w_2} = \frac{\partial w_2}{\partial x}= 0$$ (seed)
- $$\dot{w_3} = \frac{\partial w_3}{\partial{x}}= \dot{w_1}w_2 + w_1\dot{w_2}$$  
- $$\dot{w_3} = \frac{\partial w_3}{\partial{x}}= \dot{w_1}w_2 + w_1\dot{w_2}$$ 
- $$\dot{w_4} = \frac{\partial{w_4}}{\partial{x}}= \dot{w_1}cos(w_1)$$ 
- $$\dot{w_5} = \frac{\partial w_5}{\partial x}= \dot{w_3} + \dot{w_4}$$ 

Note that we need to perform the above for values $$\dot{w_1} = 0$$ and $$\dot{w_2} = 1$$ in order to calculate $$\frac{\partial f}{\partial x_2}$$

## Reverse accumulation

In reverse accumulation AD, we rely on computing **partial derivatives of the whole expression with respect to each sub-expression** over the computational graph. This means that each of the nodes in the computation graph holds the information: $$\bar{w_i} = \partial f/\partial{w_i}$$

![https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png)

*Fig 2. Reverse accumulation AD. Image source: [wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation)*

This means:

- $$\bar{w_5} = \frac{\partial f}{\partial w_5} =  1$$ (seed)
- $$\bar{w_4} = \frac{\partial f}{\partial{w_4}} = \bar{w_5} \frac{\partial w_5}{\partial w_4} = \bar{w_5}$$ 
- $$\bar{w_3} = \bar{w_5} \frac{\partial w_5}{\partial {w_3}} = \bar{w_5}$$ 
- $$\bar{w_2} = \bar{w_3}\frac{\partial w_3}{\partial w_2} = \bar{w_3} w_1$$ 
- $$\bar{w_1} = \bar{w_3}\frac{\partial w_3}{\partial w_1} = \bar{w_3} w_2$$ 
