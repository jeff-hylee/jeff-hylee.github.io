---
layout: post
title:  "Principal Component Analysis"
date:   2021-05-18 21:12:00 -0400
categories: data 
permalink: /:categories/:title:output_ext
uselatex: true
---
PCA is defined as *an orthogonal linear transformation* that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

$$ t_{k_{(i)}}$$ $$ = \mathbf{x}_{(i)} \cdot \mathbf{w}_{(k)}$$  for $$i=1,...,n$$ and $$k=1,...,l$$

Without going into too much technical detail, the computation of PCA is iterative: 

Starting from $$k = 1$$, at each step:

- we find the unitary weight vector of the $$k^{th}$$ component that maximizes the variance

$$\mathbf{w}_{(k)} = \arg\max_{\mathbf{||w||}=1}\{||\mathbf{\hat{X}}_k \mathbf{w} ||^2\} = \arg\max\{\frac{\mathbf{w}^T\mathbf{\hat{X}}^T\mathbf{\hat{X}}\mathbf{w}}{\mathbf{w}^T\mathbf{w}}\}$$ 

- we then subtract the contribution of this component and move on to the computation of the $$(k+1)^{th}$$ component.

$$\mathbf{\hat{X}}_k = \mathbf{X} - \sum_{s=1}^{k-1} \mathbf{X}\mathbf{w}_{(s)}\mathbf{w}_{(s)}^T$$
